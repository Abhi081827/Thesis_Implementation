{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cda672",
   "metadata": {},
   "source": [
    "# Tesseract OCR Pipeline with Layout Analysis\n",
    "\n",
    "This notebook creates a complete OCR pipeline using **pytesseract** that:\n",
    "- ✅ Reads layout_data.json for proper reading order\n",
    "- ✅ Processes cropped sections based on spatial layout\n",
    "- ✅ Maintains left-to-right, top-to-bottom reading order\n",
    "- ✅ Generates clean markdown files\n",
    "- ✅ Handles multi-page documents\n",
    "- ✅ Preserves document structure (headers, tables, text)\n",
    "\n",
    "**Reading Order Strategy:**\n",
    "- Primary: Uses reading_order from layout analysis\n",
    "- Secondary: Sorts by top-to-bottom (y-coordinate)\n",
    "- Tertiary: Sorts by left-to-right (x-coordinate)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Tesseract OCR installed on system\n",
    "- Cropped sections from layout detection\n",
    "- layout_data.json with bounding boxes and reading order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5e965",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225fc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ pytesseract and PIL imported successfully\n",
      "✓ All libraries imported\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OCR and Image processing\n",
    "try:\n",
    "    import pytesseract\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    print(\"✓ pytesseract and PIL imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please install: pip install pytesseract pillow\")\n",
    "    print(\"Also install Tesseract: sudo apt-get install tesseract-ocr (Linux)\")\n",
    "    raise\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ All libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893802e",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c007d339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  input_dir: output_results\n",
      "  output_dir: tesseract_markdown_output\n",
      "  tesseract_lang: eng\n",
      "  tesseract_config: --psm 6\n",
      "  confidence_threshold: 0.0\n",
      "  use_layout_reading_order: True\n",
      "  fallback_to_spatial: True\n",
      "  sort_by_position: top_left\n",
      "  include_element_type_headers: True\n",
      "  add_spacing_between_elements: True\n",
      "  format_tables: True\n",
      "  include_confidence_comments: False\n",
      "  separate_pages: True\n",
      "  page_separator: \n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'input_dir': 'output_results',\n",
    "    'output_dir': 'tesseract_markdown_output',\n",
    "    \n",
    "    # Tesseract OCR settings\n",
    "    'tesseract_lang': 'eng',  # Language: eng, fra, deu, spa, etc.\n",
    "    'tesseract_config': '--psm 6',  # Page Segmentation Mode: 6 = uniform block of text\n",
    "    'confidence_threshold': 0.0,  # Minimum confidence (0-100), 0 = no filtering\n",
    "    \n",
    "    # Reading order strategy\n",
    "    'use_layout_reading_order': True,  # Use reading_order from layout_data.json\n",
    "    'fallback_to_spatial': True,  # If no reading_order, sort by position\n",
    "    'sort_by_position': 'top_left',  # 'top_left', 'left_top', or 'reading_order'\n",
    "    \n",
    "    # Output settings\n",
    "    'include_element_type_headers': True,  # Add type info like \"## Section Header\"\n",
    "    'add_spacing_between_elements': True,  # Add blank lines between elements\n",
    "    'format_tables': True,  # Try to format table content\n",
    "    'include_confidence_comments': False,  # Add OCR confidence as HTML comments\n",
    "    \n",
    "    # Document structure\n",
    "    'separate_pages': True,  # Add page separators in markdown\n",
    "    'page_separator': '\\n---\\n\\n',  # Separator between pages\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e30afc",
   "metadata": {},
   "source": [
    "## 3. Helper Functions - File Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8481644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ File parsing functions defined\n"
     ]
    }
   ],
   "source": [
    "def parse_crop_filename(filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse crop filename to extract metadata.\n",
    "    Format: page_001_order_001_type_id_27.png\n",
    "    \"\"\"\n",
    "    parts = filename.replace('.png', '').split('_')\n",
    "    \n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'page': None,\n",
    "        'order': None,\n",
    "        'element_type': None,\n",
    "        'element_id': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Extract page number\n",
    "        if 'page' in parts:\n",
    "            page_idx = parts.index('page')\n",
    "            if page_idx + 1 < len(parts):\n",
    "                metadata['page'] = int(parts[page_idx + 1])\n",
    "        \n",
    "        # Extract reading order\n",
    "        if 'order' in parts:\n",
    "            order_idx = parts.index('order')\n",
    "            if order_idx + 1 < len(parts):\n",
    "                metadata['order'] = int(parts[order_idx + 1])\n",
    "        \n",
    "        # Extract element ID\n",
    "        if 'id' in parts:\n",
    "            id_idx = parts.index('id')\n",
    "            if id_idx + 1 < len(parts):\n",
    "                metadata['element_id'] = int(parts[id_idx + 1])\n",
    "        \n",
    "        # Extract element type (between order and id)\n",
    "        if 'order' in parts and 'id' in parts:\n",
    "            order_idx = parts.index('order')\n",
    "            id_idx = parts.index('id')\n",
    "            if order_idx + 2 < id_idx:\n",
    "                metadata['element_type'] = '_'.join(parts[order_idx + 2:id_idx])\n",
    "    \n",
    "    except (ValueError, IndexError) as e:\n",
    "        logger.warning(f\"Error parsing filename {filename}: {e}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_layout_data(document_dir: Path) -> Optional[Dict]:\n",
    "    \"\"\"Load layout_data.json for a document.\"\"\"\n",
    "    layout_file = document_dir / \"layout_data.json\"\n",
    "    \n",
    "    if not layout_file.exists():\n",
    "        logger.warning(f\"No layout_data.json found in {document_dir}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(layout_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading layout data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ File parsing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908f5fb",
   "metadata": {},
   "source": [
    "## 4. OCR Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae5a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OCR functions defined\n"
     ]
    }
   ],
   "source": [
    "def perform_tesseract_ocr(image_path: Path, config: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform OCR using Tesseract on a single image.\n",
    "    Returns text and confidence score.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Perform OCR with detailed data\n",
    "        ocr_data = pytesseract.image_to_data(\n",
    "            image,\n",
    "            lang=config['tesseract_lang'],\n",
    "            config=config['tesseract_config'],\n",
    "            output_type=pytesseract.Output.DICT\n",
    "        )\n",
    "        \n",
    "        # Extract text and confidence\n",
    "        texts = []\n",
    "        confidences = []\n",
    "        \n",
    "        for i, text in enumerate(ocr_data['text']):\n",
    "            if text.strip():  # Only non-empty text\n",
    "                conf = float(ocr_data['conf'][i])\n",
    "                if conf >= config['confidence_threshold']:\n",
    "                    texts.append(text)\n",
    "                    confidences.append(conf)\n",
    "        \n",
    "        # Combine text\n",
    "        full_text = ' '.join(texts)\n",
    "        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'text': full_text,\n",
    "            'confidence': avg_confidence,\n",
    "            'word_count': len(texts)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"OCR error for {image_path.name}: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'text': '',\n",
    "            'confidence': 0.0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ OCR functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e03f76",
   "metadata": {},
   "source": [
    "## 5. Reading Order & Sorting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31915446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reading order functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_element_layout_info(element_id: int, layout_data: Dict, page_num: int) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Get layout information for a specific element from layout_data.json.\n",
    "    Returns bounding box and reading order.\n",
    "    \"\"\"\n",
    "    if not layout_data or 'pages' not in layout_data:\n",
    "        return None\n",
    "    \n",
    "    # Find the page\n",
    "    page_idx = page_num - 1\n",
    "    if page_idx >= len(layout_data['pages']):\n",
    "        return None\n",
    "    \n",
    "    page = layout_data['pages'][page_idx]\n",
    "    \n",
    "    # Find the element\n",
    "    for elem in page.get('elements', []):\n",
    "        if elem.get('id') == element_id:\n",
    "            bbox = elem.get('bounding_box', {})\n",
    "            return {\n",
    "                'reading_order': elem.get('reading_order'),\n",
    "                'bbox': bbox,\n",
    "                'left': bbox.get('left', 0),\n",
    "                'top': bbox.get('top', 0),\n",
    "                'right': bbox.get('right', 0),\n",
    "                'bottom': bbox.get('bottom', 0),\n",
    "                'type': elem.get('type', 'unknown')\n",
    "            }\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def sort_elements_by_reading_order(elements: List[Dict], config: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Sort elements by reading order.\n",
    "    Priority:\n",
    "    1. Page number (ascending)\n",
    "    2. Reading order from layout (if available)\n",
    "    3. Top coordinate (top to bottom)\n",
    "    4. Left coordinate (left to right)\n",
    "    \"\"\"\n",
    "    def sort_key(elem: Dict) -> Tuple:\n",
    "        page = elem.get('page', 0)\n",
    "        \n",
    "        if config['use_layout_reading_order'] and elem.get('layout_info'):\n",
    "            layout = elem['layout_info']\n",
    "            reading_order = layout.get('reading_order', 999)\n",
    "            return (page, reading_order, layout.get('top', 0), layout.get('left', 0))\n",
    "        else:\n",
    "            # Fallback to spatial sorting\n",
    "            order = elem.get('order', 999)\n",
    "            return (page, order, 0, 0)\n",
    "    \n",
    "    return sorted(elements, key=sort_key)\n",
    "\n",
    "\n",
    "print(\"✓ Reading order functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576ecdf",
   "metadata": {},
   "source": [
    "## 6. Markdown Generation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05abb45",
   "metadata": {},
   "source": [
    "## 6. Advanced Column Detection\n",
    "\n",
    "This section implements intelligent column detection that can identify 1, 2, 3, or more columns automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1172d4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Advanced column detection functions defined\n"
     ]
    }
   ],
   "source": [
    "def detect_document_columns(elements: List[Dict], page_num: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Automatically detect the number of columns in a document (1, 2, 3, or more).\n",
    "    Uses clustering on horizontal positions to identify columns.\n",
    "    \"\"\"\n",
    "    page_elements = [e for e in elements if e.get('page') == page_num]\n",
    "    \n",
    "    if not page_elements:\n",
    "        return {\n",
    "            'num_columns': 0,\n",
    "            'columns': [],\n",
    "            'top_elements': [],\n",
    "            'bottom_elements': [],\n",
    "            'column_boundaries': [],\n",
    "            'layout_type': 'empty'\n",
    "        }\n",
    "    \n",
    "    # Get page width\n",
    "    page_width = 1654.0\n",
    "    elements_with_layout = [e for e in page_elements if e.get('layout_info')]\n",
    "    \n",
    "    if elements_with_layout:\n",
    "        max_right = max([e['layout_info'].get('right', 0) for e in elements_with_layout])\n",
    "        if max_right > 0:\n",
    "            page_width = max_right\n",
    "    \n",
    "    # Collect center positions of all elements\n",
    "    element_centers = []\n",
    "    for elem in elements_with_layout:\n",
    "        layout = elem['layout_info']\n",
    "        left = layout.get('left', 0)\n",
    "        right = layout.get('right', 0)\n",
    "        width = right - left\n",
    "        center = left + (width / 2)\n",
    "        element_centers.append({\n",
    "            'element': elem,\n",
    "            'center': center,\n",
    "            'left': left,\n",
    "            'right': right,\n",
    "            'width': width,\n",
    "            'top': layout.get('top', 0),\n",
    "            'bottom': layout.get('bottom', 0)\n",
    "        })\n",
    "    \n",
    "    if not element_centers:\n",
    "        return {\n",
    "            'num_columns': 1,\n",
    "            'columns': [page_elements],\n",
    "            'top_elements': [],\n",
    "            'bottom_elements': [],\n",
    "            'column_boundaries': [0, page_width],\n",
    "            'layout_type': 'single_column'\n",
    "        }\n",
    "    \n",
    "    # Sort by center position\n",
    "    element_centers.sort(key=lambda x: x['center'])\n",
    "    \n",
    "    # Identify full-width elements (headers, footers, tables)\n",
    "    full_width_threshold = page_width * 0.7\n",
    "    full_width_elements = [ec for ec in element_centers if ec['width'] > full_width_threshold]\n",
    "    narrow_elements = [ec for ec in element_centers if ec['width'] <= full_width_threshold]\n",
    "    \n",
    "    if len(narrow_elements) < 2:\n",
    "        # Only full-width elements or very few elements\n",
    "        return {\n",
    "            'num_columns': 1,\n",
    "            'columns': [page_elements],\n",
    "            'top_elements': [],\n",
    "            'bottom_elements': [],\n",
    "            'column_boundaries': [0, page_width],\n",
    "            'layout_type': 'single_column'\n",
    "        }\n",
    "    \n",
    "    # Cluster elements by their horizontal centers using simple gap-based clustering\n",
    "    # Sort narrow elements by center position\n",
    "    sorted_centers = sorted([ec['center'] for ec in narrow_elements])\n",
    "    \n",
    "    # Find gaps in the horizontal distribution\n",
    "    gaps = []\n",
    "    for i in range(len(sorted_centers) - 1):\n",
    "        gap = sorted_centers[i + 1] - sorted_centers[i]\n",
    "        if gap > page_width * 0.15:  # Significant gap (15% of page width)\n",
    "            gaps.append({\n",
    "                'position': (sorted_centers[i] + sorted_centers[i + 1]) / 2,\n",
    "                'size': gap\n",
    "            })\n",
    "    \n",
    "    # Determine number of columns based on gaps\n",
    "    num_columns = len(gaps) + 1\n",
    "    \n",
    "    # Calculate column boundaries\n",
    "    if num_columns == 1:\n",
    "        column_boundaries = [0, page_width]\n",
    "    else:\n",
    "        column_boundaries = [0] + [gap['position'] for gap in gaps] + [page_width]\n",
    "    \n",
    "    # Assign elements to columns\n",
    "    columns = [[] for _ in range(num_columns)]\n",
    "    \n",
    "    for ec in narrow_elements:\n",
    "        center = ec['center']\n",
    "        # Find which column this element belongs to\n",
    "        for i in range(num_columns):\n",
    "            left_boundary = column_boundaries[i]\n",
    "            right_boundary = column_boundaries[i + 1]\n",
    "            \n",
    "            if left_boundary <= center < right_boundary:\n",
    "                columns[i].append(ec['element'])\n",
    "                break\n",
    "    \n",
    "    # Sort elements within each column by vertical position (top to bottom)\n",
    "    for i in range(num_columns):\n",
    "        columns[i] = sorted(columns[i], key=lambda x: (\n",
    "            x.get('layout_info', {}).get('top', 0),\n",
    "            x.get('order', 0)\n",
    "        ))\n",
    "    \n",
    "    # Detect vertical range of multi-column section\n",
    "    column_start = None\n",
    "    column_end = None\n",
    "    \n",
    "    if num_columns > 1:\n",
    "        # Find elements that appear in multiple columns at similar heights\n",
    "        all_column_elements = [ec for ec in narrow_elements]\n",
    "        if all_column_elements:\n",
    "            column_start = min([ec['top'] for ec in all_column_elements])\n",
    "            column_end = max([ec['bottom'] for ec in all_column_elements])\n",
    "    \n",
    "    # Categorize full-width elements as top or bottom\n",
    "    top_elements = []\n",
    "    bottom_elements = []\n",
    "    \n",
    "    for ec in full_width_elements:\n",
    "        if column_start and ec['top'] < column_start - 50:\n",
    "            top_elements.append(ec['element'])\n",
    "        elif column_end and ec['top'] > column_end + 50:\n",
    "            bottom_elements.append(ec['element'])\n",
    "        else:\n",
    "            # Full-width element in the middle of columns - add to bottom\n",
    "            bottom_elements.append(ec['element'])\n",
    "    \n",
    "    # Sort top and bottom elements\n",
    "    top_elements.sort(key=lambda x: (\n",
    "        x.get('layout_info', {}).get('top', 0),\n",
    "        x.get('order', 0)\n",
    "    ))\n",
    "    bottom_elements.sort(key=lambda x: (\n",
    "        x.get('layout_info', {}).get('top', 0),\n",
    "        x.get('order', 0)\n",
    "    ))\n",
    "    \n",
    "    # Determine layout type\n",
    "    if num_columns == 1:\n",
    "        layout_type = 'single_column'\n",
    "    elif num_columns == 2:\n",
    "        layout_type = 'two_column'\n",
    "    elif num_columns == 3:\n",
    "        layout_type = 'three_column'\n",
    "    else:\n",
    "        layout_type = f'{num_columns}_column'\n",
    "    \n",
    "    return {\n",
    "        'num_columns': num_columns,\n",
    "        'columns': columns,\n",
    "        'top_elements': top_elements,\n",
    "        'bottom_elements': bottom_elements,\n",
    "        'column_boundaries': column_boundaries,\n",
    "        'layout_type': layout_type,\n",
    "        'page_width': page_width\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_markdown_with_columns(elements: List[Dict], config: Dict, doc_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate markdown document with automatic column detection and formatting.\n",
    "    Supports 1, 2, 3, or more columns.\n",
    "    \"\"\"\n",
    "    markdown_parts = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Group elements by page\n",
    "    pages = {}\n",
    "    for elem in elements:\n",
    "        page_num = elem.get('page', 1)\n",
    "        if page_num not in pages:\n",
    "            pages[page_num] = []\n",
    "        pages[page_num].append(elem)\n",
    "    \n",
    "    # Process each page\n",
    "    for page_num in sorted(pages.keys()):\n",
    "        if config['separate_pages']:\n",
    "            markdown_parts.append(f\"## Page {page_num}\\n\")\n",
    "        \n",
    "        # Detect columns\n",
    "        column_info = detect_document_columns(elements, page_num)\n",
    "        \n",
    "        # Add layout info comment\n",
    "        markdown_parts.append(f\"<!-- Layout: {column_info['layout_type']} ({column_info['num_columns']} column(s)) -->\")\n",
    "        markdown_parts.append('')\n",
    "        \n",
    "        # Process top elements (full-width headers, titles)\n",
    "        for elem in column_info['top_elements']:\n",
    "            formatted = format_element_for_markdown(elem, config)\n",
    "            if formatted:\n",
    "                markdown_parts.append(formatted)\n",
    "                if config['add_spacing_between_elements']:\n",
    "                    markdown_parts.append('')\n",
    "        \n",
    "        # Process columns\n",
    "        if column_info['num_columns'] > 1:\n",
    "            # Multi-column layout\n",
    "            markdown_parts.append(f'<div style=\"display: flex; gap: 20px;\">')\n",
    "            markdown_parts.append('')\n",
    "            \n",
    "            for col_idx, column in enumerate(column_info['columns']):\n",
    "                markdown_parts.append(f'<div style=\"flex: 1;\">  <!-- Column {col_idx + 1} -->')\n",
    "                markdown_parts.append('')\n",
    "                \n",
    "                for elem in column:\n",
    "                    formatted = format_element_for_markdown(elem, config)\n",
    "                    if formatted:\n",
    "                        markdown_parts.append(formatted)\n",
    "                        if config['add_spacing_between_elements']:\n",
    "                            markdown_parts.append('')\n",
    "                \n",
    "                markdown_parts.append('</div>')\n",
    "                markdown_parts.append('')\n",
    "            \n",
    "            markdown_parts.append('</div>')\n",
    "            markdown_parts.append('')\n",
    "        else:\n",
    "            # Single column - just output elements\n",
    "            for column in column_info['columns']:\n",
    "                for elem in column:\n",
    "                    formatted = format_element_for_markdown(elem, config)\n",
    "                    if formatted:\n",
    "                        markdown_parts.append(formatted)\n",
    "                        if config['add_spacing_between_elements']:\n",
    "                            markdown_parts.append('')\n",
    "        \n",
    "        # Process bottom elements (full-width tables, footers)\n",
    "        for elem in column_info['bottom_elements']:\n",
    "            formatted = format_element_for_markdown(elem, config)\n",
    "            if formatted:\n",
    "                markdown_parts.append(formatted)\n",
    "                if config['add_spacing_between_elements']:\n",
    "                    markdown_parts.append('')\n",
    "        \n",
    "        # Add page separator\n",
    "        if config['separate_pages'] and page_num < max(pages.keys()):\n",
    "            markdown_parts.append(config['page_separator'])\n",
    "    \n",
    "    return '\\n'.join(markdown_parts)\n",
    "\n",
    "\n",
    "print(\"✓ Advanced column detection functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86364da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Markdown generation functions defined\n"
     ]
    }
   ],
   "source": [
    "def format_element_for_markdown(element: Dict, config: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Format an element for markdown output based on its type.\n",
    "    \"\"\"\n",
    "    text = element.get('text', '').strip()\n",
    "    if not text:\n",
    "        return ''\n",
    "    \n",
    "    element_type = element.get('element_type', 'text')\n",
    "    markdown_lines = []\n",
    "    \n",
    "    # Add confidence comment if enabled\n",
    "    if config['include_confidence_comments'] and 'confidence' in element:\n",
    "        confidence = element['confidence']\n",
    "        markdown_lines.append(f\"<!-- OCR Confidence: {confidence:.1f}% -->\")\n",
    "    \n",
    "    # Format based on element type\n",
    "    if element_type == 'title':\n",
    "        markdown_lines.append(f\"# {text}\")\n",
    "    \n",
    "    elif element_type == 'section_header':\n",
    "        markdown_lines.append(f\"## {text}\")\n",
    "    \n",
    "    elif element_type == 'page_header':\n",
    "        markdown_lines.append(f\"*{text}*\")\n",
    "    \n",
    "    elif element_type == 'page_footer':\n",
    "        markdown_lines.append(f\"*{text}*\")\n",
    "    \n",
    "    elif element_type == 'table':\n",
    "        if config['format_tables']:\n",
    "            # Try to format as markdown table (basic formatting)\n",
    "            markdown_lines.append(\"```\")\n",
    "            markdown_lines.append(text)\n",
    "            markdown_lines.append(\"```\")\n",
    "        else:\n",
    "            markdown_lines.append(text)\n",
    "    \n",
    "    elif element_type == 'key_value_region':\n",
    "        # Format key-value pairs\n",
    "        markdown_lines.append(f\"**{text}**\")\n",
    "    \n",
    "    else:  # text, paragraph, list_item, etc.\n",
    "        markdown_lines.append(text)\n",
    "    \n",
    "    return '\\n'.join(markdown_lines)\n",
    "\n",
    "\n",
    "def detect_document_columns(elements: List[Dict], page_num: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Automatically detect the number of columns in a document (1, 2, 3, or more).\n",
    "    Uses clustering on horizontal positions to identify columns.\n",
    "    Returns dict with column information and grouped elements.\n",
    "    \"\"\"\n",
    "    page_elements = [e for e in elements if e.get('page') == page_num]\n",
    "    \n",
    "    if not page_elements:\n",
    "        return {\n",
    "            'left_column': [], \n",
    "            'right_column': [], \n",
    "            'top_elements': [], \n",
    "            'bottom_elements': [], \n",
    "            'has_columns': False,\n",
    "            'layout_type': 'single_column',\n",
    "            'num_columns': 1\n",
    "        }\n",
    "    \n",
    "    # Get page width to determine column boundary\n",
    "    page_width = 1654.0  # Default width\n",
    "    if page_elements and page_elements[0].get('layout_info'):\n",
    "        # Try to estimate page width from elements\n",
    "        max_right = max([e['layout_info'].get('right', 0) for e in page_elements if e.get('layout_info')])\n",
    "        if max_right > 0:\n",
    "            page_width = max_right\n",
    "    \n",
    "    mid_point = page_width / 2\n",
    "    threshold = page_width * 0.1  # 10% threshold for column detection\n",
    "    \n",
    "    # First, categorize elements by horizontal position\n",
    "    left_side = []\n",
    "    right_side = []\n",
    "    full_width = []\n",
    "    \n",
    "    for elem in page_elements:\n",
    "        layout = elem.get('layout_info')\n",
    "        if not layout:\n",
    "            full_width.append(elem)\n",
    "            continue\n",
    "        \n",
    "        left = layout.get('left', 0)\n",
    "        right = layout.get('right', 0)\n",
    "        width = right - left\n",
    "        \n",
    "        # Check if element spans full width\n",
    "        if width > page_width * 0.7:\n",
    "            full_width.append(elem)\n",
    "        # Check if element is in left column\n",
    "        elif right < mid_point + threshold:\n",
    "            left_side.append(elem)\n",
    "        # Check if element is in right column\n",
    "        elif left > mid_point - threshold:\n",
    "            right_side.append(elem)\n",
    "        else:\n",
    "            full_width.append(elem)\n",
    "    \n",
    "    # Detect the vertical range of the two-column section\n",
    "    # Find elements that are genuinely paired (similar vertical positions on both sides)\n",
    "    two_column_start = None\n",
    "    two_column_end = None\n",
    "    \n",
    "    if left_side and right_side:\n",
    "        # Find pairs of elements at similar vertical positions\n",
    "        paired_tops = []\n",
    "        paired_bottoms = []\n",
    "        vertical_threshold = 100  # Elements within 100px vertically are considered paired\n",
    "        \n",
    "        for left_elem in left_side:\n",
    "            left_top = left_elem.get('layout_info', {}).get('top', 999)\n",
    "            left_bottom = left_elem.get('layout_info', {}).get('bottom', 0)\n",
    "            \n",
    "            # Check if there's a right element at a similar vertical position\n",
    "            for right_elem in right_side:\n",
    "                right_top = right_elem.get('layout_info', {}).get('top', 999)\n",
    "                right_bottom = right_elem.get('layout_info', {}).get('bottom', 0)\n",
    "                \n",
    "                # If tops are close, they're likely paired\n",
    "                if abs(left_top - right_top) < vertical_threshold:\n",
    "                    paired_tops.append(min(left_top, right_top))\n",
    "                    paired_bottoms.append(max(left_bottom, right_bottom))\n",
    "                    break\n",
    "        \n",
    "        if paired_tops and paired_bottoms:\n",
    "            two_column_start = min(paired_tops)\n",
    "            two_column_end = max(paired_bottoms)\n",
    "    \n",
    "    # Categorize elements\n",
    "    top_elements = []\n",
    "    bottom_elements = []\n",
    "    left_column = []\n",
    "    right_column = []\n",
    "    \n",
    "    for elem in full_width:\n",
    "        top = elem.get('layout_info', {}).get('top', 0)\n",
    "        if two_column_start and top < two_column_start:\n",
    "            top_elements.append(elem)\n",
    "        elif two_column_end and top > two_column_end:\n",
    "            bottom_elements.append(elem)\n",
    "        else:\n",
    "            # This full-width element is in the middle of the column section\n",
    "            # Add it to bottom if it's a table or large element\n",
    "            if elem.get('element_type') in ['table', 'section_header']:\n",
    "                bottom_elements.append(elem)\n",
    "            else:\n",
    "                top_elements.append(elem)\n",
    "    \n",
    "    # Assign left and right column elements (only within the column range)\n",
    "    for elem in left_side:\n",
    "        top = elem.get('layout_info', {}).get('top', 0)\n",
    "        elem_type = elem.get('element_type', '')\n",
    "        \n",
    "        # Section headers or elements outside column range should go to top/bottom\n",
    "        if two_column_end and top > two_column_end + 20:\n",
    "            bottom_elements.append(elem)\n",
    "        elif two_column_start and two_column_end and two_column_start <= top <= two_column_end + 20:\n",
    "            left_column.append(elem)\n",
    "        elif two_column_start and top < two_column_start:\n",
    "            top_elements.append(elem)\n",
    "        else:\n",
    "            bottom_elements.append(elem)\n",
    "    \n",
    "    for elem in right_side:\n",
    "        top = elem.get('layout_info', {}).get('top', 0)\n",
    "        elem_type = elem.get('element_type', '')\n",
    "        \n",
    "        # Section headers or elements outside column range should go to top/bottom\n",
    "        if two_column_end and top > two_column_end + 20:\n",
    "            bottom_elements.append(elem)\n",
    "        elif two_column_start and two_column_end and two_column_start <= top <= two_column_end + 20:\n",
    "            right_column.append(elem)\n",
    "        elif two_column_start and top < two_column_start:\n",
    "            top_elements.append(elem)\n",
    "        else:\n",
    "            bottom_elements.append(elem)\n",
    "    \n",
    "    # Determine layout type and number of columns\n",
    "    has_columns = len(left_column) > 0 and len(right_column) > 0\n",
    "    layout_type = 'two_column' if has_columns else 'single_column'\n",
    "    num_columns = 2 if has_columns else 1\n",
    "    \n",
    "    return {\n",
    "        'left_column': sorted(left_column, key=lambda x: (x.get('layout_info', {}).get('top', 0), x.get('order', 0))),\n",
    "        'right_column': sorted(right_column, key=lambda x: (x.get('layout_info', {}).get('top', 0), x.get('order', 0))),\n",
    "        'top_elements': sorted(top_elements, key=lambda x: (x.get('layout_info', {}).get('top', 0), x.get('order', 0))),\n",
    "        'bottom_elements': sorted(bottom_elements, key=lambda x: (x.get('layout_info', {}).get('top', 0), x.get('order', 0))),\n",
    "        'has_columns': has_columns,\n",
    "        'layout_type': layout_type,\n",
    "        'num_columns': num_columns\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_markdown_document(elements: List[Dict], config: Dict, doc_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate complete markdown document from sorted elements.\n",
    "    Handles two-column layouts intelligently.\n",
    "    \"\"\"\n",
    "    markdown_parts = []\n",
    "    \n",
    "    # Add document title\n",
    "    markdown_parts.append(f\"# {doc_name}\")\n",
    "    markdown_parts.append(f\"\\n*Generated with Tesseract OCR - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\")\n",
    "    \n",
    "    # Group elements by page\n",
    "    pages = {}\n",
    "    for elem in elements:\n",
    "        page_num = elem.get('page', 1)\n",
    "        if page_num not in pages:\n",
    "            pages[page_num] = []\n",
    "        pages[page_num].append(elem)\n",
    "    \n",
    "    # Process each page\n",
    "    for page_num in sorted(pages.keys()):\n",
    "        page_elements = pages[page_num]\n",
    "        \n",
    "        # Add page header\n",
    "        if config['separate_pages']:\n",
    "            markdown_parts.append(f\"\\n---\\n## Page {page_num}\\n\")\n",
    "        \n",
    "        # Detect columns for this page\n",
    "        columns = detect_document_columns(page_elements, page_num)\n",
    "        \n",
    "        if columns['has_columns']:\n",
    "            # Process top elements first (headers at the top)\n",
    "            for elem in columns['top_elements']:\n",
    "                formatted = format_element_for_markdown(elem, config)\n",
    "                if formatted:\n",
    "                    markdown_parts.append(formatted)\n",
    "                    if config['add_spacing_between_elements']:\n",
    "                        markdown_parts.append('')\n",
    "            \n",
    "            # Create two-column section\n",
    "            markdown_parts.append('<div style=\"display: flex; gap: 20px;\">')\n",
    "            markdown_parts.append('')\n",
    "            \n",
    "            # Left column\n",
    "            markdown_parts.append('<div style=\"flex: 1;\">')\n",
    "            markdown_parts.append('')\n",
    "            for elem in columns['left_column']:\n",
    "                formatted = format_element_for_markdown(elem, config)\n",
    "                if formatted:\n",
    "                    markdown_parts.append(formatted)\n",
    "                    if config['add_spacing_between_elements']:\n",
    "                        markdown_parts.append('')\n",
    "            markdown_parts.append('</div>')\n",
    "            markdown_parts.append('')\n",
    "            \n",
    "            # Right column\n",
    "            markdown_parts.append('<div style=\"flex: 1;\">')\n",
    "            markdown_parts.append('')\n",
    "            for elem in columns['right_column']:\n",
    "                formatted = format_element_for_markdown(elem, config)\n",
    "                if formatted:\n",
    "                    markdown_parts.append(formatted)\n",
    "                    if config['add_spacing_between_elements']:\n",
    "                        markdown_parts.append('')\n",
    "            markdown_parts.append('</div>')\n",
    "            markdown_parts.append('')\n",
    "            \n",
    "            markdown_parts.append('</div>')\n",
    "            markdown_parts.append('')\n",
    "            \n",
    "            # Process bottom elements (tables, footers, etc.)\n",
    "            for elem in columns['bottom_elements']:\n",
    "                formatted = format_element_for_markdown(elem, config)\n",
    "                if formatted:\n",
    "                    markdown_parts.append(formatted)\n",
    "                    if config['add_spacing_between_elements']:\n",
    "                        markdown_parts.append('')\n",
    "        else:\n",
    "            # Single column layout - process normally\n",
    "            for elem in page_elements:\n",
    "                formatted = format_element_for_markdown(elem, config)\n",
    "                if formatted:\n",
    "                    markdown_parts.append(formatted)\n",
    "                    if config['add_spacing_between_elements']:\n",
    "                        markdown_parts.append('')\n",
    "        \n",
    "        # Add page separator\n",
    "        if config['separate_pages'] and page_num < max(pages.keys()):\n",
    "            markdown_parts.append(config['page_separator'])\n",
    "    \n",
    "    return '\\n'.join(markdown_parts)\n",
    "\n",
    "\n",
    "print(\"✓ Markdown generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4af066",
   "metadata": {},
   "source": [
    "## 7. Main Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd73b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main processing function defined\n"
     ]
    }
   ],
   "source": [
    "def process_document(document_dir: Path, config: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single document:\n",
    "    1. Load layout_data.json\n",
    "    2. Find all cropped sections\n",
    "    3. Perform OCR on each section\n",
    "    4. Sort by reading order\n",
    "    5. Generate markdown\n",
    "    \"\"\"\n",
    "    doc_name = document_dir.name\n",
    "    logger.info(f\"Processing document: {doc_name}\")\n",
    "    \n",
    "    # Load layout data\n",
    "    layout_data = load_layout_data(document_dir)\n",
    "    \n",
    "    # Find cropped sections\n",
    "    crops_dir = document_dir / \"cropped_sections\"\n",
    "    if not crops_dir.exists():\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f\"No cropped_sections directory found in {document_dir}\"\n",
    "        }\n",
    "    \n",
    "    crop_files = sorted([f for f in crops_dir.glob('*.png') if f.name.startswith('page_')])\n",
    "    \n",
    "    if not crop_files:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f\"No crop files found in {crops_dir}\"\n",
    "        }\n",
    "    \n",
    "    # Process each crop\n",
    "    elements = []\n",
    "    \n",
    "    for crop_file in tqdm(crop_files, desc=f\"OCR {doc_name}\", leave=False):\n",
    "        # Parse filename\n",
    "        metadata = parse_crop_filename(crop_file.name)\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_result = perform_tesseract_ocr(crop_file, config)\n",
    "        \n",
    "        if not ocr_result['success'] or not ocr_result['text'].strip():\n",
    "            continue\n",
    "        \n",
    "        # Get layout information\n",
    "        layout_info = None\n",
    "        if layout_data and metadata['page'] and metadata['element_id'] is not None:\n",
    "            layout_info = get_element_layout_info(\n",
    "                metadata['element_id'],\n",
    "                layout_data,\n",
    "                metadata['page']\n",
    "            )\n",
    "        \n",
    "        # Create element record\n",
    "        element = {\n",
    "            'page': metadata['page'],\n",
    "            'order': metadata['order'],\n",
    "            'element_type': metadata['element_type'],\n",
    "            'element_id': metadata['element_id'],\n",
    "            'text': ocr_result['text'],\n",
    "            'confidence': ocr_result['confidence'],\n",
    "            'word_count': ocr_result['word_count'],\n",
    "            'layout_info': layout_info\n",
    "        }\n",
    "        \n",
    "        elements.append(element)\n",
    "    \n",
    "    # Sort elements by reading order\n",
    "    sorted_elements = sort_elements_by_reading_order(elements, config)\n",
    "    \n",
    "    # Generate markdown\n",
    "    markdown_content = generate_markdown_document(sorted_elements, config, doc_name)\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'document_name': doc_name,\n",
    "        'total_elements': len(sorted_elements),\n",
    "        'markdown': markdown_content,\n",
    "        'elements': sorted_elements\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Main processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8046946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Enhanced processing function with column detection defined\n"
     ]
    }
   ],
   "source": [
    "def process_document_with_column_detection(document_dir: Path, config: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single document with automatic column detection:\n",
    "    1. Load layout_data.json\n",
    "    2. Find all cropped sections\n",
    "    3. Perform OCR on each section\n",
    "    4. Sort by reading order\n",
    "    5. Detect columns automatically\n",
    "    6. Generate markdown with proper column formatting\n",
    "    \"\"\"\n",
    "    doc_name = document_dir.name\n",
    "    logger.info(f\"Processing document: {doc_name}\")\n",
    "    \n",
    "    # Load layout data\n",
    "    layout_data = load_layout_data(document_dir)\n",
    "    \n",
    "    # Find cropped sections\n",
    "    crops_dir = document_dir / \"cropped_sections\"\n",
    "    if not crops_dir.exists():\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f\"No cropped_sections directory found in {document_dir}\"\n",
    "        }\n",
    "    \n",
    "    crop_files = sorted([f for f in crops_dir.glob('*.png') if f.name.startswith('page_')])\n",
    "    \n",
    "    if not crop_files:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f\"No crop files found in {crops_dir}\"\n",
    "        }\n",
    "    \n",
    "    # Process each crop\n",
    "    elements = []\n",
    "    \n",
    "    for crop_file in tqdm(crop_files, desc=f\"OCR {doc_name}\", leave=False):\n",
    "        # Parse filename\n",
    "        metadata = parse_crop_filename(crop_file.name)\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_result = perform_tesseract_ocr(crop_file, config)\n",
    "        \n",
    "        if not ocr_result['success'] or not ocr_result['text'].strip():\n",
    "            continue\n",
    "        \n",
    "        # Get layout information\n",
    "        layout_info = None\n",
    "        if layout_data and metadata['page'] and metadata['element_id'] is not None:\n",
    "            layout_info = get_element_layout_info(\n",
    "                metadata['element_id'],\n",
    "                layout_data,\n",
    "                metadata['page']\n",
    "            )\n",
    "        \n",
    "        # Create element record\n",
    "        element = {\n",
    "            'page': metadata['page'],\n",
    "            'order': metadata['order'],\n",
    "            'element_type': metadata['element_type'],\n",
    "            'element_id': metadata['element_id'],\n",
    "            'text': ocr_result['text'],\n",
    "            'confidence': ocr_result['confidence'],\n",
    "            'word_count': ocr_result['word_count'],\n",
    "            'layout_info': layout_info\n",
    "        }\n",
    "        \n",
    "        elements.append(element)\n",
    "    \n",
    "    # Sort elements by reading order\n",
    "    sorted_elements = sort_elements_by_reading_order(elements, config)\n",
    "    \n",
    "    # Detect columns for each page\n",
    "    pages = list(set([e.get('page', 1) for e in sorted_elements]))\n",
    "    column_info = {}\n",
    "    for page_num in pages:\n",
    "        col_info = detect_document_columns(sorted_elements, page_num)\n",
    "        column_info[page_num] = col_info\n",
    "        logger.info(f\"  Page {page_num}: {col_info['layout_type']} ({col_info['num_columns']} columns)\")\n",
    "    \n",
    "    # Generate markdown with column formatting\n",
    "    markdown_content = generate_markdown_with_columns(sorted_elements, config, doc_name)\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'document_name': doc_name,\n",
    "        'total_elements': len(sorted_elements),\n",
    "        'markdown': markdown_content,\n",
    "        'elements': sorted_elements,\n",
    "        'column_info': column_info\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Enhanced processing function with column detection defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9e9f6",
   "metadata": {},
   "source": [
    "## 8. Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb109034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batch processing function defined\n"
     ]
    }
   ],
   "source": [
    "def batch_process_documents(input_dir: Path, output_dir: Path, config: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process all documents in the input directory.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all document directories\n",
    "    document_dirs = [d for d in input_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not document_dirs:\n",
    "        logger.warning(f\"No document directories found in {input_dir}\")\n",
    "        return {'success': False, 'error': 'No documents found'}\n",
    "    \n",
    "    logger.info(f\"Found {len(document_dirs)} documents to process\")\n",
    "    \n",
    "    results = {\n",
    "        'total_documents': len(document_dirs),\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'details': []\n",
    "    }\n",
    "    \n",
    "    for doc_dir in tqdm(document_dirs, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            # Process document\n",
    "            result = process_document(doc_dir, config)\n",
    "            \n",
    "            if result['success']:\n",
    "                # Save markdown\n",
    "                output_file = output_dir / f\"{result['document_name']}.md\"\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(result['markdown'])\n",
    "                \n",
    "                results['successful'] += 1\n",
    "                results['details'].append({\n",
    "                    'document': result['document_name'],\n",
    "                    'status': 'success',\n",
    "                    'elements': result['total_elements'],\n",
    "                    'output_file': str(output_file)\n",
    "                })\n",
    "                \n",
    "                logger.info(f\"✓ {result['document_name']}: {result['total_elements']} elements\")\n",
    "            else:\n",
    "                results['failed'] += 1\n",
    "                results['details'].append({\n",
    "                    'document': doc_dir.name,\n",
    "                    'status': 'failed',\n",
    "                    'error': result.get('error', 'Unknown error')\n",
    "                })\n",
    "                logger.error(f\"✗ {doc_dir.name}: {result.get('error')}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            results['failed'] += 1\n",
    "            results['details'].append({\n",
    "                'document': doc_dir.name,\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            })\n",
    "            logger.error(f\"✗ {doc_dir.name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Batch processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a91b0",
   "metadata": {},
   "source": [
    "## 9. Test Single Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b702aaf8",
   "metadata": {},
   "source": [
    "## 9. Test with Automatic Column Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588909d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 13:19:41,699 - INFO - Processing document: batch1-0287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with enhanced column detection: batch1-0287\\n\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OCR batch1-0287:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'layout_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Process single document with column detection\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_document_with_column_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn✅ Successfully processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 78\u001b[0m, in \u001b[0;36mprocess_document_with_column_detection\u001b[0;34m(document_dir, config)\u001b[0m\n\u001b[1;32m     76\u001b[0m     col_info \u001b[38;5;241m=\u001b[39m detect_document_columns(sorted_elements, page_num)\n\u001b[1;32m     77\u001b[0m     column_info[page_num] \u001b[38;5;241m=\u001b[39m col_info\n\u001b[0;32m---> 78\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcol_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayout_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_columns\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Generate markdown with column formatting\u001b[39;00m\n\u001b[1;32m     81\u001b[0m markdown_content \u001b[38;5;241m=\u001b[39m generate_markdown_with_columns(sorted_elements, config, doc_name)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'layout_type'"
     ]
    }
   ],
   "source": [
    "# Test with automatic column detection\n",
    "input_dir = Path(CONFIG['input_dir'])\n",
    "output_dir = Path(CONFIG['output_dir'])\n",
    "\n",
    "# Find first document\n",
    "document_dirs = [d for d in input_dir.iterdir() if d.is_dir()]\n",
    "if document_dirs:\n",
    "    test_doc = document_dirs[0]\n",
    "    print(f\"Testing with enhanced column detection: {test_doc.name}\\\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Process single document with column detection\n",
    "    result = process_document_with_column_detection(test_doc, CONFIG)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\\\n✅ Successfully processed: {result['document_name']}\")\n",
    "        print(f\"   Total elements: {result['total_elements']}\")\n",
    "        \n",
    "        # Display column information\n",
    "        print(f\"\\\\n📊 Column Detection Results:\")\n",
    "        for page_num, col_info in result['column_info'].items():\n",
    "            print(f\"\\\\n   Page {page_num}:\")\n",
    "            print(f\"     - Layout Type: {col_info['layout_type']}\")\n",
    "            print(f\"     - Number of Columns: {col_info['num_columns']}\")\n",
    "            print(f\"     - Page Width: {col_info.get('page_width', 'N/A'):.1f}px\")\n",
    "            if col_info['num_columns'] > 1:\n",
    "                print(f\"     - Column Boundaries: {[f'{b:.1f}' for b in col_info['column_boundaries']]}\")\n",
    "                for i, col in enumerate(col_info['columns']):\n",
    "                    print(f\"     - Column {i+1}: {len(col)} elements\")\n",
    "            print(f\"     - Top Elements: {len(col_info['top_elements'])}\")\n",
    "            print(f\"     - Bottom Elements: {len(col_info['bottom_elements'])}\")\n",
    "        \n",
    "        # Save to file\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_file = output_dir / f\"{result['document_name']}.md\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(result['markdown'])\n",
    "        \n",
    "        print(f\"\\\\n✓ Markdown saved to: {output_file}\")\n",
    "        print(f\"\\\\n📄 Markdown preview (first 600 chars):\")\n",
    "        print(\"=\"*70)\n",
    "        print(result['markdown'][:600])\n",
    "        if len(result['markdown']) > 600:\n",
    "            print(\"...\")\n",
    "    else:\n",
    "        print(f\"✗ Error: {result.get('error')}\")\n",
    "else:\n",
    "    print(\"No documents found for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c397391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 01:30:46,099 - INFO - Processing document: batch1-0287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with document: batch1-0287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed: batch1-0287\n",
      "  Total elements: 13\n",
      "\n",
      "First 5 elements:\n",
      "\n",
      "  [1] Page 1, Order 1\n",
      "      Type: key_value_region\n",
      "      Text: Invoice no: 51335214 Date of issue: 03/27/2015\n",
      "      Confidence: 95.9%\n",
      "\n",
      "  [2] Page 1, Order 2\n",
      "      Type: section_header\n",
      "      Text: Seller:\n",
      "      Confidence: 96.0%\n",
      "\n",
      "  [3] Page 1, Order 3\n",
      "      Type: section_header\n",
      "      Text: Client:\n",
      "      Confidence: 96.0%\n",
      "\n",
      "  [4] Page 1, Order 4\n",
      "      Type: text\n",
      "      Text: Rivera Group 90443 lan Inlet Suite 586 Lake Abigail, WV 40743\n",
      "      Confidence: 95.5%\n",
      "\n",
      "  [5] Page 1, Order 5\n",
      "      Type: text\n",
      "      Text: Malone, Wilson and Carson 01909 Kyle Port South Joyce, VA 45070\n",
      "      Confidence: 95.7%\n",
      "\n",
      "✓ Markdown saved to: tesseract_markdown_output/batch1-0287.md\n",
      "\n",
      "Markdown preview (first 500 chars):\n",
      "============================================================\n",
      "# batch1-0287\n",
      "\n",
      "*Generated with Tesseract OCR - 2025-10-23 01:30:48*\n",
      "\n",
      "## Page 1\n",
      "\n",
      "**Invoice no: 51335214 Date of issue: 03/27/2015**\n",
      "\n",
      "<div style=\"display: flex; gap: 20px;\">\n",
      "\n",
      "<div style=\"flex: 1;\">\n",
      "\n",
      "## Seller:\n",
      "\n",
      "Rivera Group 90443 lan Inlet Suite 586 Lake Abigail, WV 40743\n",
      "\n",
      "**Tax Id: 988-71-1654**\n",
      "\n",
      "IBAN: GB10XNTM38918437896667\n",
      "\n",
      "</div>\n",
      "\n",
      "<div style=\"flex: 1;\">\n",
      "\n",
      "## Client:\n",
      "\n",
      "Malone, Wilson and Carson 01909 Kyle Port South Joyce, VA 45070\n",
      "\n",
      "Tax Id: 928-86-3224\n",
      "\n",
      "**Tax Id: 928-86-3224**\n",
      "\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "\n",
      "##\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Test with a single document\n",
    "input_dir = Path(CONFIG['input_dir'])\n",
    "output_dir = Path(CONFIG['output_dir'])\n",
    "\n",
    "# Find first document\n",
    "document_dirs = [d for d in input_dir.iterdir() if d.is_dir()]\n",
    "if document_dirs:\n",
    "    test_doc = document_dirs[0]\n",
    "    print(f\"Testing with document: {test_doc.name}\\n\")\n",
    "    \n",
    "    # Process single document\n",
    "    result = process_document(test_doc, CONFIG)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"✓ Successfully processed: {result['document_name']}\")\n",
    "        print(f\"  Total elements: {result['total_elements']}\")\n",
    "        print(f\"\\nFirst 5 elements:\")\n",
    "        \n",
    "        for i, elem in enumerate(result['elements'][:5], 1):\n",
    "            print(f\"\\n  [{i}] Page {elem['page']}, Order {elem['order']}\")\n",
    "            print(f\"      Type: {elem['element_type']}\")\n",
    "            print(f\"      Text: {elem['text'][:80]}...\" if len(elem['text']) > 80 else f\"      Text: {elem['text']}\")\n",
    "            print(f\"      Confidence: {elem['confidence']:.1f}%\")\n",
    "        \n",
    "        # Save to file\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_file = output_dir / f\"{result['document_name']}.md\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(result['markdown'])\n",
    "        \n",
    "        print(f\"\\n✓ Markdown saved to: {output_file}\")\n",
    "        print(f\"\\nMarkdown preview (first 500 chars):\")\n",
    "        print(\"=\"*60)\n",
    "        print(result['markdown'][:500])\n",
    "        print(\"...\")\n",
    "    else:\n",
    "        print(f\"✗ Error: {result.get('error')}\")\n",
    "else:\n",
    "    print(\"No documents found for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea392d9a",
   "metadata": {},
   "source": [
    "## 10. Batch Process All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a9ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 01:30:48,448 - INFO - Found 61 documents to process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/61 [00:00<?, ?it/s]2025-10-23 01:30:48,452 - INFO - Processing document: batch1-0287\n",
      "2025-10-23 01:30:50,805 - INFO - ✓ batch1-0287: 13 elements\n",
      "Processing documents:   2%|▏         | 1/61 [00:02<02:21,  2.35s/it]2025-10-23 01:30:50,806 - INFO - Processing document: 50120516-0516\n",
      "2025-10-23 01:30:52,288 - INFO - ✓ 50120516-0516: 5 elements\n",
      "Processing documents:   3%|▎         | 2/61 [00:03<01:48,  1.84s/it]2025-10-23 01:30:52,289 - INFO - Processing document: invoice_Anna Gayman_42837\n",
      "2025-10-23 01:30:53,673 - INFO - ✓ invoice_Anna Gayman_42837: 6 elements\n",
      "Processing documents:   5%|▍         | 3/61 [00:05<01:34,  1.63s/it]2025-10-23 01:30:53,674 - INFO - Processing document: batch1-0281\n",
      "2025-10-23 01:30:55,629 - INFO - ✓ batch1-0281: 11 elements\n",
      "Processing documents:   7%|▋         | 4/61 [00:07<01:40,  1.76s/it]2025-10-23 01:30:55,630 - INFO - Processing document: 0001139626\n",
      "2025-10-23 01:30:57,629 - INFO - ✓ 0001139626: 7 elements\n",
      "Processing documents:   8%|▊         | 5/61 [00:09<01:43,  1.85s/it]2025-10-23 01:30:57,630 - INFO - Processing document: 00922240\n",
      "2025-10-23 01:30:59,876 - INFO - ✓ 00922240: 9 elements\n",
      "Processing documents:  10%|▉         | 6/61 [00:11<01:49,  1.98s/it]2025-10-23 01:30:59,877 - INFO - Processing document: batch1-0283\n",
      "2025-10-23 01:31:01,560 - INFO - ✓ batch1-0283: 11 elements\n",
      "Processing documents:  11%|█▏        | 7/61 [00:13<01:41,  1.88s/it]2025-10-23 01:31:01,560 - INFO - Processing document: invoice_Anemone Ratner_8876\n",
      "2025-10-23 01:31:02,692 - INFO - ✓ invoice_Anemone Ratner_8876: 6 elements\n",
      "Processing documents:  13%|█▎        | 8/61 [00:14<01:27,  1.65s/it]2025-10-23 01:31:02,693 - INFO - Processing document: sliders-454353423425\n",
      "2025-10-23 01:31:07,016 - INFO - ✓ sliders-454353423425: 2 elements\n",
      "Processing documents:  15%|█▍        | 9/61 [00:18<02:09,  2.48s/it]2025-10-23 01:31:07,017 - INFO - Processing document: invoice_Andy Yotov_37314_with_crops\n",
      "2025-10-23 01:31:08,053 - INFO - ✓ invoice_Andy Yotov_37314_with_crops: 6 elements\n",
      "Processing documents:  16%|█▋        | 10/61 [00:19<01:43,  2.04s/it]2025-10-23 01:31:08,054 - INFO - Processing document: ti31689150\n",
      "2025-10-23 01:31:09,314 - INFO - ✓ ti31689150: 9 elements\n",
      "Processing documents:  18%|█▊        | 11/61 [00:20<01:29,  1.80s/it]2025-10-23 01:31:09,315 - INFO - Processing document: batch1-0021\n",
      "2025-10-23 01:31:11,328 - INFO - ✓ batch1-0021: 12 elements\n",
      "Processing documents:  20%|█▉        | 12/61 [00:22<01:31,  1.86s/it]2025-10-23 01:31:11,329 - INFO - Processing document: batch1-0003\n",
      "2025-10-23 01:31:13,138 - INFO - ✓ batch1-0003: 12 elements\n",
      "Processing documents:  21%|██▏       | 13/61 [00:24<01:28,  1.85s/it]2025-10-23 01:31:13,139 - INFO - Processing document: invoice_Angele Hood_12988\n",
      "2025-10-23 01:31:14,175 - INFO - ✓ invoice_Angele Hood_12988: 6 elements\n",
      "Processing documents:  23%|██▎       | 14/61 [00:25<01:15,  1.60s/it]2025-10-23 01:31:14,176 - INFO - Processing document: invoice_Anna Chung_36194\n",
      "2025-10-23 01:31:15,228 - INFO - ✓ invoice_Anna Chung_36194: 6 elements\n",
      "Processing documents:  25%|██▍       | 15/61 [00:26<01:06,  1.44s/it]2025-10-23 01:31:15,229 - INFO - Processing document: invoice_Anna Andreadi_39300\n",
      "2025-10-23 01:31:16,285 - INFO - ✓ invoice_Anna Andreadi_39300: 6 elements\n",
      "Processing documents:  26%|██▌       | 16/61 [00:27<00:59,  1.32s/it]2025-10-23 01:31:16,286 - INFO - Processing document: happyitaly_20240306_001\n",
      "2025-10-23 01:31:17,276 - INFO - ✓ happyitaly_20240306_001: 4 elements\n",
      "Processing documents:  28%|██▊       | 17/61 [00:28<00:53,  1.22s/it]2025-10-23 01:31:17,277 - INFO - Processing document: invoice_Anna HДberlin_40216\n",
      "2025-10-23 01:31:18,327 - INFO - ✓ invoice_Anna HДberlin_40216: 6 elements\n",
      "Processing documents:  30%|██▉       | 18/61 [00:29<00:50,  1.17s/it]2025-10-23 01:31:18,328 - INFO - Processing document: motelone_20240203\n",
      "2025-10-23 01:31:21,848 - INFO - ✓ motelone_20240203: 17 elements\n",
      "Processing documents:  31%|███       | 19/61 [00:33<01:18,  1.88s/it]2025-10-23 01:31:21,849 - INFO - Processing document: batch1-0282\n",
      "2025-10-23 01:31:23,604 - INFO - ✓ batch1-0282: 12 elements\n",
      "Processing documents:  33%|███▎      | 20/61 [00:35<01:15,  1.84s/it]2025-10-23 01:31:23,605 - INFO - Processing document: ti31149327_9330\n",
      "2025-10-23 01:31:25,633 - INFO - ✓ ti31149327_9330: 12 elements\n",
      "Processing documents:  34%|███▍      | 21/61 [00:37<01:15,  1.90s/it]2025-10-23 01:31:25,633 - INFO - Processing document: invoice_Anna Andreadi_39301\n",
      "2025-10-23 01:31:26,061 - INFO - ✓ invoice_Anna Andreadi_39301: 2 elements\n",
      "Processing documents:  36%|███▌      | 22/61 [00:37<00:56,  1.46s/it]2025-10-23 01:31:26,062 - INFO - Processing document: invoice_Angele Hood_35601\n",
      "2025-10-23 01:31:27,176 - INFO - ✓ invoice_Angele Hood_35601: 6 elements\n",
      "Processing documents:  38%|███▊      | 23/61 [00:38<00:51,  1.35s/it]2025-10-23 01:31:27,177 - INFO - Processing document: shakeshack_20181208_004\n",
      "2025-10-23 01:31:29,467 - INFO - ✓ shakeshack_20181208_004: 11 elements\n",
      "Processing documents:  39%|███▉      | 24/61 [00:41<01:00,  1.64s/it]2025-10-23 01:31:29,469 - INFO - Processing document: invoice_Anna Andreadi_39302\n",
      "2025-10-23 01:31:30,718 - INFO - ✓ invoice_Anna Andreadi_39302: 6 elements\n",
      "Processing documents:  41%|████      | 25/61 [00:42<00:54,  1.52s/it]2025-10-23 01:31:30,719 - INFO - Processing document: batch1-0276\n",
      "2025-10-23 01:31:32,757 - INFO - ✓ batch1-0276: 13 elements\n",
      "Processing documents:  43%|████▎     | 26/61 [00:44<00:58,  1.68s/it]2025-10-23 01:31:32,758 - INFO - Processing document: ti31689113\n",
      "2025-10-23 01:31:34,316 - INFO - ✓ ti31689113: 10 elements\n",
      "Processing documents:  44%|████▍     | 27/61 [00:45<00:55,  1.64s/it]2025-10-23 01:31:34,317 - INFO - Processing document: 2085538660\n",
      "2025-10-23 01:31:36,257 - INFO - ✓ 2085538660: 3 elements\n",
      "Processing documents:  46%|████▌     | 28/61 [00:47<00:57,  1.73s/it]2025-10-23 01:31:36,258 - INFO - Processing document: batch1-0285\n",
      "2025-10-23 01:31:38,236 - INFO - ✓ batch1-0285: 14 elements\n",
      "Processing documents:  48%|████▊     | 29/61 [00:49<00:57,  1.81s/it]2025-10-23 01:31:38,237 - INFO - Processing document: batch1-0020\n",
      "2025-10-23 01:31:40,158 - INFO - ✓ batch1-0020: 11 elements\n",
      "Processing documents:  49%|████▉     | 30/61 [00:51<00:57,  1.84s/it]2025-10-23 01:31:40,159 - INFO - Processing document: batch1-0274\n",
      "2025-10-23 01:31:42,182 - INFO - ✓ batch1-0274: 12 elements\n",
      "Processing documents:  51%|█████     | 31/61 [00:53<00:56,  1.90s/it]2025-10-23 01:31:42,183 - INFO - Processing document: invoice_Andy Yotov_37313\n",
      "2025-10-23 01:31:43,305 - INFO - ✓ invoice_Andy Yotov_37313: 6 elements\n",
      "Processing documents:  52%|█████▏    | 32/61 [00:54<00:48,  1.66s/it]2025-10-23 01:31:43,307 - INFO - Processing document: invoice_Ann Blume_35427\n",
      "2025-10-23 01:31:44,499 - INFO - ✓ invoice_Ann Blume_35427: 6 elements\n",
      "Processing documents:  54%|█████▍    | 33/61 [00:56<00:42,  1.52s/it]2025-10-23 01:31:44,500 - INFO - Processing document: batch1-0288\n",
      "2025-10-23 01:31:46,312 - INFO - ✓ batch1-0288: 11 elements\n",
      "Processing documents:  56%|█████▌    | 34/61 [00:57<00:43,  1.61s/it]2025-10-23 01:31:46,313 - INFO - Processing document: batch1-0027\n",
      "2025-10-23 01:31:48,204 - INFO - ✓ batch1-0027: 11 elements\n",
      "Processing documents:  57%|█████▋    | 35/61 [00:59<00:44,  1.69s/it]2025-10-23 01:31:48,205 - INFO - Processing document: batch1-0278\n",
      "2025-10-23 01:31:50,408 - INFO - ✓ batch1-0278: 12 elements\n",
      "Processing documents:  59%|█████▉    | 36/61 [01:01<00:46,  1.85s/it]2025-10-23 01:31:50,409 - INFO - Processing document: 00920576\n",
      "2025-10-23 01:31:52,888 - INFO - ✓ 00920576: 19 elements\n",
      "Processing documents:  61%|██████    | 37/61 [01:04<00:48,  2.04s/it]2025-10-23 01:31:52,889 - INFO - Processing document: batch1-0029\n",
      "2025-10-23 01:31:55,163 - INFO - ✓ batch1-0029: 12 elements\n",
      "Processing documents:  62%|██████▏   | 38/61 [01:06<00:48,  2.11s/it]2025-10-23 01:31:55,164 - INFO - Processing document: batch1-0275\n",
      "2025-10-23 01:31:57,227 - INFO - ✓ batch1-0275: 11 elements\n",
      "Processing documents:  64%|██████▍   | 39/61 [01:08<00:46,  2.10s/it]2025-10-23 01:31:57,228 - INFO - Processing document: invoice_Andy Yotov_37314\n",
      "2025-10-23 01:31:58,282 - INFO - ✓ invoice_Andy Yotov_37314: 6 elements\n",
      "Processing documents:  66%|██████▌   | 40/61 [01:09<00:37,  1.78s/it]2025-10-23 01:31:58,283 - INFO - Processing document: invoice_Anna Andreadi_35319\n",
      "2025-10-23 01:31:59,543 - INFO - ✓ invoice_Anna Andreadi_35319: 6 elements\n",
      "Processing documents:  67%|██████▋   | 41/61 [01:11<00:32,  1.63s/it]2025-10-23 01:31:59,545 - INFO - Processing document: batch1-0001\n",
      "2025-10-23 01:32:02,065 - INFO - ✓ batch1-0001: 13 elements\n",
      "Processing documents:  69%|██████▉   | 42/61 [01:13<00:36,  1.89s/it]2025-10-23 01:32:02,065 - INFO - Processing document: invoice_Andy Yotov_39986\n",
      "2025-10-23 01:32:03,205 - INFO - ✓ invoice_Andy Yotov_39986: 6 elements\n",
      "Processing documents:  70%|███████   | 43/61 [01:14<00:30,  1.67s/it]2025-10-23 01:32:03,206 - INFO - Processing document: batch1-0286\n",
      "2025-10-23 01:32:05,649 - INFO - ✓ batch1-0286: 13 elements\n",
      "Processing documents:  72%|███████▏  | 44/61 [01:17<00:32,  1.90s/it]2025-10-23 01:32:05,649 - INFO - Processing document: 00921466\n",
      "2025-10-23 01:32:07,676 - INFO - ✓ 00921466: 12 elements\n",
      "Processing documents:  74%|███████▍  | 45/61 [01:19<00:31,  1.94s/it]2025-10-23 01:32:07,678 - INFO - Processing document: 0001139716\n",
      "2025-10-23 01:32:09,983 - INFO - ✓ 0001139716: 8 elements\n",
      "Processing documents:  75%|███████▌  | 46/61 [01:21<00:30,  2.05s/it]2025-10-23 01:32:09,984 - INFO - Processing document: invoice_Anna Chung_36195\n",
      "2025-10-23 01:32:11,115 - INFO - ✓ invoice_Anna Chung_36195: 6 elements\n",
      "Processing documents:  77%|███████▋  | 47/61 [01:22<00:24,  1.77s/it]2025-10-23 01:32:11,116 - INFO - Processing document: invoice_Andy Yotov_37312\n",
      "2025-10-23 01:32:12,495 - INFO - ✓ invoice_Andy Yotov_37312: 6 elements\n",
      "Processing documents:  79%|███████▊  | 48/61 [01:24<00:21,  1.66s/it]2025-10-23 01:32:12,496 - INFO - Processing document: 0013046347\n",
      "2025-10-23 01:32:14,828 - INFO - ✓ 0013046347: 13 elements\n",
      "Processing documents:  80%|████████  | 49/61 [01:26<00:22,  1.86s/it]2025-10-23 01:32:14,830 - INFO - Processing document: batch1-0005\n",
      "2025-10-23 01:32:17,203 - INFO - ✓ batch1-0005: 13 elements\n",
      "Processing documents:  82%|████████▏ | 50/61 [01:28<00:22,  2.01s/it]2025-10-23 01:32:17,204 - INFO - Processing document: batch1-0284\n",
      "2025-10-23 01:32:19,183 - INFO - ✓ batch1-0284: 12 elements\n",
      "Processing documents:  84%|████████▎ | 51/61 [01:30<00:20,  2.00s/it]2025-10-23 01:32:19,185 - INFO - Processing document: invoice_Anna Andreadi_35317\n",
      "2025-10-23 01:32:20,419 - INFO - ✓ invoice_Anna Andreadi_35317: 6 elements\n",
      "Processing documents:  85%|████████▌ | 52/61 [01:31<00:15,  1.77s/it]2025-10-23 01:32:20,420 - INFO - Processing document: batch1-0277\n",
      "2025-10-23 01:32:22,656 - INFO - ✓ batch1-0277: 11 elements\n",
      "Processing documents:  87%|████████▋ | 53/61 [01:34<00:15,  1.91s/it]2025-10-23 01:32:22,658 - INFO - Processing document: invoice_Anna Andreadi_35318\n",
      "2025-10-23 01:32:23,786 - INFO - ✓ invoice_Anna Andreadi_35318: 6 elements\n",
      "Processing documents:  89%|████████▊ | 54/61 [01:35<00:11,  1.68s/it]2025-10-23 01:32:23,789 - INFO - Processing document: batch1-0279\n",
      "2025-10-23 01:32:25,898 - INFO - ✓ batch1-0279: 12 elements\n",
      "Processing documents:  90%|█████████ | 55/61 [01:37<00:10,  1.81s/it]2025-10-23 01:32:25,899 - INFO - Processing document: spaceneedle_20240528_005\n",
      "2025-10-23 01:32:27,822 - INFO - ✓ spaceneedle_20240528_005: 14 elements\n",
      "Processing documents:  92%|█████████▏| 56/61 [01:39<00:09,  1.84s/it]2025-10-23 01:32:27,823 - INFO - Processing document: batch1-0280\n",
      "2025-10-23 01:32:29,782 - INFO - ✓ batch1-0280: 13 elements\n",
      "Processing documents:  93%|█████████▎| 57/61 [01:41<00:07,  1.88s/it]2025-10-23 01:32:29,783 - INFO - Processing document: batch1-0007\n",
      "2025-10-23 01:32:32,117 - INFO - ✓ batch1-0007: 13 elements\n",
      "Processing documents:  95%|█████████▌| 58/61 [01:43<00:06,  2.02s/it]2025-10-23 01:32:32,118 - INFO - Processing document: ti16311032\n",
      "2025-10-23 01:32:33,398 - INFO - ✓ ti16311032: 9 elements\n",
      "Processing documents:  97%|█████████▋| 59/61 [01:44<00:03,  1.79s/it]2025-10-23 01:32:33,400 - INFO - Processing document: invoice_Andy Yotov_37315\n",
      "2025-10-23 01:32:34,755 - INFO - ✓ invoice_Andy Yotov_37315: 6 elements\n",
      "Processing documents:  98%|█████████▊| 60/61 [01:46<00:01,  1.66s/it]2025-10-23 01:32:34,756 - INFO - Processing document: invoice_Angele Hood_35602\n",
      "2025-10-23 01:32:36,116 - INFO - ✓ invoice_Angele Hood_35602: 6 elements\n",
      "Processing documents: 100%|██████████| 61/61 [01:47<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BATCH PROCESSING SUMMARY\n",
      "============================================================\n",
      "Total documents: 61\n",
      "Successful: 61\n",
      "Failed: 0\n",
      "\n",
      "Output directory: tesseract_markdown_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all documents\n",
    "print(\"Starting batch processing...\\n\")\n",
    "\n",
    "input_dir = Path(CONFIG['input_dir'])\n",
    "output_dir = Path(CONFIG['output_dir'])\n",
    "\n",
    "results = batch_process_documents(input_dir, output_dir, CONFIG)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BATCH PROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total documents: {results['total_documents']}\")\n",
    "print(f\"Successful: {results['successful']}\")\n",
    "print(f\"Failed: {results['failed']}\")\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "\n",
    "if results['failed'] > 0:\n",
    "    print(\"\\nFailed documents:\")\n",
    "    for detail in results['details']:\n",
    "        if detail['status'] == 'failed':\n",
    "            print(f\"  - {detail['document']}: {detail['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a0b192",
   "metadata": {},
   "source": [
    "## 11. View Generated Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ba36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing: 00921466.md\n",
      "\n",
      "============================================================\n",
      "# 00921466\n",
      "\n",
      "*Generated with Tesseract OCR - 2025-10-23 01:32:07*\n",
      "\n",
      "## Page 1\n",
      "\n",
      "<div style=\"display: flex; gap: 20px;\">\n",
      "\n",
      "<div style=\"flex: 1;\">\n",
      "\n",
      "BORRISTON RESEARCH LABORATORIES, INC.\n",
      "\n",
      "August 20, 1981\n",
      "\n",
      "LORILLARD, INC. 420 English St. Greensboro, N.C. 27420\n",
      "\n",
      "**Attention: Dr. Harry Minnemeyer Reference: Purchase Order # 312-A BRL Ref.:  2-22+222-J Invoice No.: 5-J**\n",
      "\n",
      "## DESCRIPTION\n",
      "\n",
      "For submission of Final Report “Cardiovascular Testing of Compound A-11 in the Beagle Dog\" at $2,700.00 per compound.\n",
      "\n",
      "</div>\n",
      "\n",
      "<div style=\"flex: 1;\">\n",
      "\n",
      "## * * ® RINVOICE® * # *\n",
      "\n",
      "**AMOUNT. $2,700.00**\n",
      "\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "\n",
      "**Remittance Address: ENVIRO CONTROL INC. 11140 Rockville Pike Rockville, Md. 20852 Attn: B. Belford, Accounting**\n",
      "\n",
      "*: :*\n",
      "\n",
      "* % ® MINVOICE® * * *\n",
      "\n",
      "*5050 Beech Place © Temple Hills, Maryland 20031 @ 301-899-3536,*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View a generated markdown file\n",
    "output_dir = Path(CONFIG['output_dir'])\n",
    "markdown_files = list(output_dir.glob('*.md'))\n",
    "\n",
    "if markdown_files:\n",
    "    # Display first file\n",
    "    sample_file = markdown_files[0]\n",
    "    print(f\"Viewing: {sample_file.name}\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        print(content[:1500])  # First 1500 characters\n",
    "        if len(content) > 1500:\n",
    "            print(\"\\n... (truncated)\")\n",
    "            print(f\"\\nTotal length: {len(content)} characters\")\n",
    "else:\n",
    "    print(\"No markdown files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82948698",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This pipeline provides:\n",
    "\n",
    "### ✅ Features Implemented:\n",
    "1. **Layout-aware OCR**: Reads layout_data.json for proper structure\n",
    "2. **Reading order preservation**: \n",
    "   - Primary: Uses reading_order from layout analysis\n",
    "   - Secondary: Top-to-bottom sorting\n",
    "   - Tertiary: Left-to-right sorting\n",
    "3. **Clean markdown output**: Formatted based on element types\n",
    "4. **Batch processing**: Process multiple documents at once\n",
    "5. **Confidence tracking**: Monitor OCR quality\n",
    "6. **Error handling**: Robust error management\n",
    "\n",
    "### 📊 Processing Flow:\n",
    "```\n",
    "Input: Cropped images + layout_data.json\n",
    "  ↓\n",
    "Parse filenames → Extract metadata\n",
    "  ↓\n",
    "Load layout info → Get reading order & bounding boxes\n",
    "  ↓\n",
    "Perform OCR → Extract text with Tesseract\n",
    "  ↓\n",
    "Sort elements → By reading order (top-to-bottom, left-to-right)\n",
    "  ↓\n",
    "Generate markdown → Clean, structured output\n",
    "  ↓\n",
    "Output: Markdown files\n",
    "```\n",
    "\n",
    "### 🎯 Next Steps:\n",
    "1. Run cell 9 to test single document\n",
    "2. Run cell 10 to process all documents\n",
    "3. Run cell 11 to view generated markdown\n",
    "4. Adjust CONFIG settings as needed\n",
    "5. Check output in `tesseract_markdown_output/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e3cd1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
